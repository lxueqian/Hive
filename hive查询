1. 尽量不要用*
2. 列别名 可以不加as
3. 算术运算符 select sal+1 from table
4. 常用函数：5个，一定会触发mapreduce任务 count min max sum avg
5. between and
   like  %：多个字符 _：一个字符
   rlike 可以接java中的正则表达试
6. where后不能写聚合函数，having可以，和group by连用
7. join语句只能支持等值连接，不支持非等值连接；支持全外连接 full join；on后面不支持or
8. 使用表名前缀可以提高效率
9. hive建表建宽表较好，可以减少计算需求，因为join效率比较低
10. 笛卡儿积 省略连接条件 相当于两个表行数相×
11. order by 全局排序 一个reducer；也可以按照别名排序；多个列排序
12. 每个mapreduce内部排序：sort by 每个区的数据是随机算法得到的（防止数据倾斜）
    设置reduce个数：set mapreduce.job.reduce=3；
    查看recude个数：set mapreduce.job.reduces;
    将查询结果导入文件中 ： insert overwrite local directory '/path' select * from emp sort by deptno desc;
13. 如果没有写local，需要在hadoop fs -cat /path 找到相应文件
14. 分区排序：distribute by 结合sort by使用，写在sort by之前 分区内的数据按照指定字段哈希分区
    与sort by类似，只有设置多个reduce才能看出效果
    > set mapreduce.job.reduces=3;
    > insert overwrite local directory '/opt/.../distribute-result' select * from emp 
    > distribute by deptno sort by empno desc; 每个部门按照工资排序，并保存结果至...
15. 分区排序：cluster by --distribute by和sort by 后面的字段相同
    对于50各部门但是只有3个分区的情况比较有用；只能升序排，没有 asc或者desc
16. 分桶及抽样排序
    分区针对数据存储路径(多个文件夹），分桶针对的是数据文件（同一个文件夹多个文件）
    分区是针对新字段；分桶针对已有字段
    1）先创建分桶表，通过直接导入数据文件的方式
    > create table stu buck(id int, name string)
    > clustered by(id)
    > into 4 buckets
    > row format delimited fields terminated by '\t';
    
    > desc formatted stu_buck;
    > load data local inpath '/path..' into table stu_back;
    > trancate table 名
    > alter table name drop partition(key=value),partition(key=value),....
    结果没有分桶
    2）先创建一个普通表，然后通过子查询的方法将数据导入分桶表
    需要设置一个属性：set hive.enforce.bucketing=true; set mapreduce.job.reduces=-1;
    再插入数据 insert into table stu_back select id,name from stu;
    
    > select * from stu_buck tablesample(bucket 1 out of 4 on id);
    tablesample(bucket x out of y) y必须是table总桶数的因子或者倍数，决定抽样比例；
    x表示从哪个bucket开始抽取，如果需要抽取多个分区，以后的分区号为当前加上y
    x必须小于y
17. date_format('2019-12-13','yyyy-MM-dd HH:mm:ss')
    date_add('2019-12-13',5/-5)
    date_sub()
    datediff('2019-12-13','2019-12-14')
    regexp_replace('2019/12/13','/','-') 替换
18. > select concat(deptno,'-',dname) from table; 多列合成一列  合并一行的数据 把多个字段拼成一个
    > select concat_ws('-',field1,field2,...) 必须是string类型field1-field2-field3
    > select collect_set(field)  把该字段的值去重变成一个array 把多行合成一行 是一个聚合函数 需要和group by连用
    > concat_ws('|',collect_set(ti.name)) name group by 把一组的数据化成一行
    > explode(字段） 把字段中每个值拆成多行
    > select movie,category from movie_info lateral view explode(category) table_name as category_name
    把拆分出来的多行与原来的表关联起来
19. hive架构
    元数据 存在mysql中
    原始数据 存在HDFS中
    解析器、编译器、优化器、执行器
    本质：将HQL转化成Mapreduce程序
    mysql安装是服务级别的安装，在root下安装
20. 优化方式
    1）fetch抓取： 全局查找、字段查找、limit查找不走mapreduce 需要设置hive.fetch.task.conversion=more
    2) 本地模式：set hive.exec.mode.local.auto=true, 当输入文件数小于4个，大小小于128M
    3）小表 join 大表： set hive.auto.convert.join = true 会自动对大小表的join做调整
    4）大表 join 大表： 
      空key过滤：
      空key转换：空key的数据很多，但是不能过滤，若不操作会导致数据倾斜，因为同一个key的数据会发给一个reducer，所以可以
      给空key赋随机值
      从输出网址中查看是否出现数据倾斜 
      case when n.id is null then concat('hive',rand()) else n.id end 降低数据倾斜，但是总体时间会变慢
    5）mapjoin：set hive.auto.convert.join = true; set hive.mapjoin.smalltable.filesize=250000000(大小表的阈值设置25M）
      可以用mapjoin把小表全部加载到内存在map端进行join，避免reducer操作
    6）group by: 可以开启map端进行聚合 hive.map.aggr = true; 
                 在map端进行聚合的条目 hive.groupby.mapaggr.checkinterval=100000
                 有数据倾斜的时候进行负载均衡 hive.groupby.skewindata=true：会生成两个MR任务，第一个MR任务中Map的输出结果
                 会随机分到reduce中，每个reduce部分聚合并输出结果，但是想通过key的可能被分到不同reduce中，然后再进入第二个
                 MR任务完成最终聚合
    7）count(distinct) 需要一个reduce完成，需要处理的量比较大，一般先用group by在count的方式
       select count(id) from (select id from table group by id)
       实际中
       ,count(distinct u) as user_pv -- 曝光用户量
        ,count(distinct qpid) as video_pv -- 内容曝光个数
        ,count(1) as pv -- 内容曝光次数
        ,count(1)/count(distinct u) as avg_pv -- 人均曝光次数
        这样直接写有时也可以，如果不行再调整
    8) 避免笛卡儿积：join必须加有效的on,hive只能用一个reducer完成笛卡儿积
    9）行列过滤：只拿需要的列；连表先写where条件
    10）动态分区调整： 
        set hive.exec.dynamic.partition=true;
        set hive.exec.dynamic.partition.mode=nonstrict;  --启用动态分区
        insert into table dept_par partition(deptno) --这里不指定分区名就是动态分区，指定就是静态分区
        select loc,name,deptno from dept; 分区字段根据最后一个select字段决定
    11）分区/分桶
    12）map前将小文件合并 参数已经默认设置
        复杂文件增加map数量
        调整reducer数量  set mapreduce,hob.reduces = 15; 若为-1根据输入的原始数据量/256得到
        hive.exec.reducers.max = 1009最大的reducer数量
21. hadoop压缩
    set hive.exec.compress.intermediate=true; hive中间传输数据压缩功能
    set mapreduce.map.output.compress = true; map端输出压缩功能
    set mapreduce.map.output.compress.codec = org.apache.hadoop.io.compress.SnappyCodec;
    set hive.exec.compress.output=true; hive最终输出数据压缩功能
22. 行存储和列存储 select * select column
23. hive存入数据：load/insert put(修复数据/增加分区）
24. hive会把一个查询转化为多个阶段（mr阶段、抽样阶段、合并阶段、limit阶段),默认会一次执行一个阶段，但是有些可以并行；
    并行执行：set hive.exec.parallel=true； set hive.exeu.parallel.thread.number = 16;同一个sql允许的最大并行度
25. SerDe 允许Hive 从table 读取数据，并以任何自定义格式将其写回HDFS
26. Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能
27. 可以使用explain命令，查看语法树，即hive先将sql语法转成对应的语法树 还可以使用explain extended 查看更多信息
28. COALESCE ( expression,value1,value2……,valuen) 
    COALESCE()函数的第一个参数expression为待检测的表达式，而其后的参数个数不定。
    COALESCE()函数将会返回包括expression在内的所有参数中的第一个非空表达式。
    COALESCE()函数可以用来完成几乎所有的空值处理，不过在很多数据库系统中都提供了它的简化版，这些简化版中只接受两个变量，其参数格式如下： 
    MYSQL: 
      IFNULL(expression,value) 
    MSSQLServer: 
      ISNULL(expression,value) 
    Oracle: 
      NVL(expression,value) 
29. UDF(User-Defined-Function)，一对一。Hive内置字符串，数学函数，时间函数都是
       这种类型。大多数情况下编写对应功能的处理函数就能满足需求。如：concat, split, length ,rand等。
       这种UDF主要有两种写法：继承实现UDF类和继承GenericUDF类（通用UDF）
       
    UDTF(User-Defined Table-Generating Functions) 一对多。
       如explode, 通常配合Lateral View使用，实现列转行的功能。parse_url_tuple将一列转为多列
       
    UDAF(User Defined Aggregation Function)多对一。
       一般配合group by使用。主要用于累加操作，常见的函数有max， min， count， sum，collect_set
       这种UDF主要有两种写法：继承实现 UDAF类和继承实现AbstractGenericUDAFResolver类。
       
  UDF函数可以直接应用于select语句，对查询结构做格式化处理后，再输出内容。
  编写UDF函数的时候需要注意一下几点：
    a）自定义UDF需要继承org.apache.hadoop.hive.ql.UDF。
    b）需要实现evaluate函。 函数名只能使用evaluate
    c）evaluate函数支持重载。
    4）在hive命令行创建函数 添加jar--add jar path 创建function--create [temporary] function add5 as '全类名'
    5）在hive删除函数
 
 UDTF函数
    1）需要继承GenericUDTF类
    2）需要实现三种方法
        @override 表示子类重写了父类的方法 
        inistialize 初始化方法 数据类型与sql不同 输出数据的列名/输出数据的类型定义
        process 业务逻辑：获取数据-获取分隔符-遍历写出-将数据放在集合中-写出forward(datalist)
        close
    3）打成jar包上传
    4）将jar包添加到hive的class path下
    5）创建临时函数与开发好的javaclass关联 

30. 查看hive自有函数：
  show functions --查看已有函数 describe function concat -- 查看函数用法 describe function extended concat -- 查看函数的用户和示例
31. 字符串函数
  LENGTH( string str ) 长度
  TRIM( string str ) 去除空格
  UPPER( string str ), UCASE( string str ) 
  SUBSTR( string source_str, int start_position [,int length]  ) 返回字符串的一部分
  SUBSTRING( string source_str, int start_position [,int length]  ) SUBSTR('hadoop',4) returns 'oop'  SUBSTR('hadoop',4,2) returns 'oo'
  SPLIT( string str, string pat )和python一样分割字符串   SPLIT('hive:hadoop',':') returns ["hive","hadoop"]
  LPAD( string str, int len, string pad ) 用指定字符串在左侧填充到指定长度
  FIND_IN_SET( string search_string, string source_string_list ) 返回目标字符串位置
  CONCAT( string str1, string str2... ) 链接字符串
  CONCAT_WS( string delimiter, string str1, string str2... )  用第一个字符链接后面所有字符串
32. 窗口函数
  over() 给每条每条每条数据都开窗，限定一个聚合函数的作用范围 
    group by 后面才执行窗口函数
    group by只能多对一，无法给每条数据都产生一个结果
    sum() over(distribute by cost sort by orderdate) 按人按日期据累加
    over()其他限制窗口大小的函数————current row当前行 row between  and  current row
    查看每个人的购买总量 select name,orderdate,cost,sum(cost) over(distribute by name sort by orderdate)
    查看每个人上次购买时间 select name,orderdate,cost,lag(orderdate,1,'1970-01-01') over(distribute by name sort by orderdate)
    lead(field,n,default) 后n行的某个数据
    利用lag可以计算两天连续买的人
    ntile(n) 把所有数据行平均分组  
    查看前20% 时间的订单信息 select ntile_5 from (
    select name,cost,orderdate,ntile(5) over(order by orderdate) ntile_5 from business) where ntile_5=1;
  over前面
    rank() 排序相同会重复，总数不变 并列第一，没有第二了
    row_number() 没有并列
    dense_rank() 排序相同会重复，总数会减少，并列第一有第二
 33. hive命令行中可以执行hadoop的命令 dfs
 34. hive数据仓库默认位置： hdfs上：/user/hive/warehouse
     hive-site.xml conf文件夹下面
     hive-default.xml
     -hiveconf param=value 优先级最高
 35. hive数据类型：bigint double string int map<string,int> struct<street:string,city:string> array<string>
 36. DDL 库和表的增删改查 --操作元数据
    1）内外部表
       内部表（管理表）--元数据(mysql中） 能被删除
       外部表--实际数据（hdfs)不能被删除
       转化：> desc formatted student; 查询表类型
             > alter table student set tblproperties('EXTERNAL'='TRUE'/'FALSE'); 区分大小写
    2）建表
       create [external] table [if not exists] tablename [(col type,...)] 
       partitioned by (col type,..)
       row format row_format（默认是SerDe-序列化反序列化)(delimited fileds terminated by char)
       location path (指定表在hdfs的存储位置）
       
       外部表，仅仅记录数据所在路径，不对数据进行转移 元数据被删除实际数据不动
       内部表，会将数据转移到数据仓库指定路径， 元数据和实际数据一起删除
     3) 修改表
        alter table tblname change column oldname newname type; 改变原有列名或属性
        alter table tblname replace column (newname type); 只能替换所有列
        alter table tblname add columns (col type,col typ...); 增加列，在所有列的后面（partition列前）
        update tblname set col='' [where]
     4）删除表
        truncate table tblname，只能清空管理表--清空表数据
        drop tabel tblname (同mysql) 删除表结构
           
 37. DML 数据操作语言：数据导入/导出 --操作实际数据
     本地插入：load data [local本地/hdfs] inpath 'path' [overwrite覆盖] into table haha; 
     通过查询插入：insert overwrite/into(覆盖/不覆盖） table student partition (dt=) select... insert overwrite ...(可连多个）
     查询创建表并加载数据：create table if not exists student as select name,score from student0; 
     创建表通过location指定加载数据位置：create table tablename() location 'hadoop path';
     import数据到指定hive表：只能对先export的数据导出的
     
     插入 静态分区 select后不能加分区字段
          动态分区 select后需要加分区字段，必须保证是最后一个

 38.数据导出：
    insert导出： insert overwrite [local] directory 'path' row format delimited fields terminated by '\t' 
    导出到本地： hive -e '' > path
 39.truncate 清空表，只能清空管理表
 40.分区表：
    对应一个HDFS文件系统上独立的文件夹，文件夹下面是该分区的所有数据文件
    Hive分区就是分目录
    谓词下推：先走where再选择
    > alter table tblname add partition(dt='2019-12-12') partition(dt='2019-12-14')
    > alter table tblname drop partition(dt=''),partition(dt='');
    > show partitions tblname;
    > desc formatted tblname
    
    二级分区：
    partitioned by (col type1,col2 type2);
    三种上传数据添加分区的方式：
    1）上传后修复： > dfs -mkdir -p path 建立文件夹
                   > dfs -put source target 添加原始数据
                   > msck repair table tblname; 添加元数据信息
    2) 上传后添加分区：前两步同上
                   > alter table tblname add partition(); 添加分区
    3）创建文件夹后load数据到分区：
                    > load data local inpath  into table tblname partition();
    4) 建立一个分区表，然后直接insert数据到分区：
                    > insert overwrite table tblname partition(dt='') select...;
                    
  
     
     
     
     
 
  
  
