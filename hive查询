1.尽量不要用*
2.列别名 可以不加as
3.算术运算符 select sal+1 from table
4.常用函数：5个，一定会触发mapreduce任务 count min max sum avg
5.between and
  like  %：多个字符 _：一个字符
  rlike 可以接java中的正则表达试
6.where后不能写聚合函数，having可以，和group by连用
7.join语句只能支持等值连接，不支持非等值连接；支持全外连接 full join；on后面不支持or
8. 使用表名前缀可以提高效率
9. hive建表建宽表较好，可以减少计算需求，因为join效率比较低
10. 笛卡儿积 省略连接条件 相当于两个表行数相×
11. order by 全局排序 一个reducer；也可以按照别名排序；多个列排序
12. 每个mapreduce内部排序：sort by 每个区的数据是随机算法得到的（防止数据倾斜）
    设置reduce个数：set mapreduce.job.reduce=3；
    查看recude个数：set mapreduce.job.reduces;
    将查询结果导入文件中 ： insert overwrite local directory '/path' select * from emp sort by deptno desc;
13. 如果没有写local，需要在hadoop fs -cat /path 找到相应文件
14. 分区排序：distribute by 结合sort by使用，写在sort by之前 分区内的数据按照指定字段哈希分区
    与sort by类似，只有设置多个reduce才能看出效果
    > set mapreduce.job.reduces=3;
    > insert overwrite local directory '/opt/.../distribute-result' select * from emp 
    > distribute by deptno sort by empno desc; 每个部门按照工资排序，并保存结果至...
15. 分区排序：cluster by --distribute by和sort by 后面的字段相同
    对于50各部门但是只有3个分区的情况比较有用；只能升序排，没有 asc或者desc
16. 分桶及抽样排序
    分区针对数据存储路径(多个文件夹），分桶针对的是数据文件（同一个文件夹多个文件）
    分区是针对新字段；分桶针对已有字段
    1）先创建分桶表，通过直接导入数据文件的方式
    > create table stu buck(id int, name string)
    > clustered by(id)
    > into 4 buckets
    > row format delimited fields terminated by '\t';
    
    > desc formatted stu_buck;
    > load data local inpath '/path..' into table stu_back;
    > trancate table 名
    > alter table name drop partition(key=value),partition(key=value),....
    结果没有分桶
    2）先创建一个普通表，然后通过子查询的方法将数据导入分桶表
    需要设置一个属性：set hive.enforce.bucketing=true; set mapreduce.job.reduces=-1;
    再插入数据 insert into table stu_back select id,name from stu;
    
    > select * from stu_buck tablesample(bucket 1 out of 4 on id);
    tablesample(bucket x out of y) y必须是table总桶数的因子或者倍数，决定抽样比例；
    x表示从哪个bucket开始抽取，如果需要抽取多个分区，以后的分区号为当前加上y
    x必须小于y
17. date_format('2019-12-13','yyyy-MM-dd HH:mm:ss')
    date_add('2019-12-13',5/-5)
    date_sub()
    datediff('2019-12-13','2019-12-14')
    regexp_replace('2019/12/13','/','-') 替换
18. > select concat(deptno,'-',dname) from table; 多列合成一列  合并一行的数据 把多个字段拼成一个
    > select concat_ws('-',field1,field2,...) 必须是string类型field1-field2-field3
    > select collect_set(field)  把该字段的值去重变成一个array 把多行合成一行 是一个聚合函数 需要和group by连用
    > concat_ws('|',collect_set(ti.name)) name group by 把一组的数据化成一行
    > explode(字段） 把字段中每个值拆成多行
    > select movie,category from movie_info lateral view explode(category) table_name as category_name
    把拆分出来的多行与原来的表关联起来
19. 窗口函数
    把原数据和聚合后的数据一起展示--group做不到
20. 优化方式
    1）fetch抓取： 全局查找、字段查找、limit查找不走mapreduce 需要设置hive.fetch.task.conversion=more
    2) 本地模式：set hive.exec.mode.local.auto=true, 当输入文件数小于4个，大小小于128M
    3）小表 join 大表： set hive.auto.convert.join = true 会自动对大小表的join做调整
    4）大表 join 大表： 
      空key过滤：
      空key转换：空key的数据很多，但是不能过滤，若不操作会导致数据倾斜，因为同一个key的数据会发给一个reducer，所以可以
      给空key赋随机值
      从输出网址中查看是否出现数据倾斜 
      case when n.id is null then concat('hive',rand()) else n.id end 降低数据倾斜，但是总体时间会变慢
    5）mapjoin：set hive.auto.convert.join = true; set hive.mapjoin.smalltable.filesize=250000000(大小表的阈值设置25M）
      可以用mapjoin把小表全部加载到内存在map端进行join，避免reducer操作
    6）group by: 可以开启map端进行聚合 hive.map.aggr = true; 
                 在map端进行聚合的条目 hive.groupby.mapaggr.checkinterval=100000
                 有数据倾斜的时候进行负载均衡 hive.groupby.skewindata=true：会生成两个MR任务，第一个MR任务中Map的输出结果
                 会随机分到reduce中，每个reduce部分聚合并输出结果，但是想通过key的可能被分到不同reduce中，然后再进入第二个
                 MR任务完成最终聚合
    7）count(distinct) 需要一个reduce完成，需要处理的量比较大，一般先用group by在count的方式
       select count(id) from (select id from table group by id)
       实际中
       ,count(distinct u) as user_pv -- 曝光用户量
        ,count(distinct qpid) as video_pv -- 内容曝光个数
        ,count(1) as pv -- 内容曝光次数
        ,count(1)/count(distinct u) as avg_pv -- 人均曝光次数
        这样直接写有时也可以，如果不行再调整
    8) 避免笛卡儿积：join必须加有效的on,hive只能用一个reducer完成笛卡儿积
    9）行列过滤：只拿需要的列；连表先写where条件
    10）动态分区调整： 
        set hive.exec.dynamic.partition=true;
        set hive.exec.dynamic.partition.mode=nonstrict;  --启用动态分区
        insert into table dept_par partition(deptno) --这里不指定分区名就是动态分区，指定就是静态分区
        select loc,name,deptno from dept; 分区字段根据最后一个select字段决定
    11）分区/分桶
    12）map前将小文件合并 参数已经默认设置
        复杂文件增加map数量
        调整reducer数量  set mapreduce,hob.reduces = 15; 若为-1根据输入的原始数据量/256得到
        hive.exec.reducers.max = 1009最大的reducer数量
21. hadoop压缩
    set hive.exec.compress.intermediate=true; hive中间传输数据压缩功能
    set mapreduce.map.output.compress = true; map端输出压缩功能
    set mapreduce.map.output.compress.codec = org.apache.hadoop.io.compress.SnappyCodec;
    set hive.exec.compress.output=true; hive最终输出数据压缩功能
22. 行存储和列存储 select * select column
23. hive存入数据：load/insert put(修复数据/增加分区）
24. hive会把一个查询转化为多个阶段（mr阶段、抽样阶段、合并阶段、limit阶段),默认会一次执行一个阶段，但是有些可以并行；
    并行执行：set hive.exec.parallel=true； set hive.exeu.parallel.thread.number = 16;同一个sql允许的最大并行度
25. SerDe 允许Hive 从table 读取数据，并以任何自定义格式将其写回HDFS
26. Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能
27. 可以使用explain命令，查看语法树，即hive先将sql语法转成对应的语法树 还可以使用explain extended 查看更多信息
28. COALESCE ( expression,value1,value2……,valuen) 
    COALESCE()函数的第一个参数expression为待检测的表达式，而其后的参数个数不定。
    COALESCE()函数将会返回包括expression在内的所有参数中的第一个非空表达式。
    COALESCE()函数可以用来完成几乎所有的空值处理，不过在很多数据库系统中都提供了它的简化版，这些简化版中只接受两个变量，其参数格式如下： 
    MYSQL: 
      IFNULL(expression,value) 
    MSSQLServer: 
      ISNULL(expression,value) 
    Oracle: 
      NVL(expression,value) 
29. UDF(User-Defined-Function)，用户自定义函数对数据进行处理。  单独处理一行，输出也是以行输出。Hive内置字符串，数学函数，时间函数都是
       这种类型。大多数情况下编写对应功能的处理函数就能满足需求。如：concat, split, length ,rand等。
       这种UDF主要有两种写法：继承实现UDF类和继承GenericUDF类（通用UDF）
    UDTF(User-Defined Table-Generating Functions) 用来解决输入一行输出多行(On-to-many maping) 的需求。
       一般配合group by使用。主要用于累加操作，常见的函数有max， min， count， sum，collect_set
       这种UDF主要有两种写法：继承实现 UDAF类和继承实现AbstractGenericUDAFResolver类。
    UDAF(User Defined Aggregation Function)用户自定义聚合函数，操作多个数据行，产生一个数据行。
       如explode, 通常配合Lateral View使用，实现列转行的功能。parse_url_tuple将一列转为多列
       
  UDF函数可以直接应用于select语句，对查询结构做格式化处理后，再输出内容。
  编写UDF函数的时候需要注意一下几点：
    a）自定义UDF需要继承org.apache.hadoop.hive.ql.UDF。
    b）需要实现evaluate函。
    c）evaluate函数支持重载。
30. 查看hive自有函数：
  show functions --查看已有函数 describe function concat -- 查看函数用法 describe function extended concat -- 查看函数的用户和示例
31. 字符串函数
  LENGTH( string str ) 长度
  TRIM( string str ) 去除空格
  UPPER( string str ), UCASE( string str ) 
  SUBSTR( string source_str, int start_position [,int length]  ) 返回字符串的一部分
  SUBSTRING( string source_str, int start_position [,int length]  ) SUBSTR('hadoop',4) returns 'oop'  SUBSTR('hadoop',4,2) returns 'oo'
  SPLIT( string str, string pat )和python一样分割字符串   SPLIT('hive:hadoop',':') returns ["hive","hadoop"]
  LPAD( string str, int len, string pad ) 用指定字符串在左侧填充到指定长度
  FIND_IN_SET( string search_string, string source_string_list ) 返回目标字符串位置
  CONCAT( string str1, string str2... ) 链接字符串
  CONCAT_WS( string delimiter, string str1, string str2... )  用第一个字符链接后面所有字符串
  
