1. 尽量不要用*
2. 列别名 可以不加as
3. 算术运算符 select sal+1 from table
4. 常用函数：5个，一定会触发mapreduce任务 count min max sum avg
5. between and
   like  %：多个字符 _：一个字符
   rlike 可以接java中的正则表达试
6. where后不能写聚合函数，having可以，和group by连用
7. join语句只能支持等值连接，不支持非等值连接；支持全外连接 full join；on后面不支持or
   left semi join 是in/exists子查询的实现方式，子句右边的表只能在on子句设置过滤条件；只能显示左表字段
   相当于select * from A where A.key in (select B.key from B)
   select * from A left semi join B on (A.key=B.key)
   
8. 使用表名前缀可以提高效率
9. hive建表建宽表较好，可以减少计算需求，因为join效率比较低
10. 笛卡儿积 省略连接条件 相当于两个表行数相×
11. order by 全局排序 一个reducer；也可以按照别名排序；多个列排序
12. 每个mapreduce内部排序：sort by 每个区的数据是随机算法得到的（防止数据倾斜）
    设置reduce个数：set mapreduce.job.reduce=3；
    查看recude个数：set mapreduce.job.reduces;
    将查询结果导入文件中 ： insert overwrite local directory '/path' select * from emp sort by deptno desc;
13. 如果没有写local，需要在hadoop fs -cat /path 找到相应文件
14. 分区排序：distribute by 结合sort by使用，写在sort by之前 分区内的数据按照指定字段哈希分区
    与sort by类似，只有设置多个reduce才能看出效果
    > set mapreduce.job.reduces=3;
    > insert overwrite local directory '/opt/.../distribute-result' select * from emp 
    > distribute by deptno sort by empno desc; 每个部门按照工资排序，并保存结果至...
15. 分区排序：cluster by --distribute by和sort by 后面的字段相同
    对于50各部门但是只有3个分区的情况比较有用；只能升序排，没有 asc或者desc
16. 分桶及抽样排序
    分区针对数据存储路径(多个文件夹），分桶针对的是数据文件（同一个文件夹多个文件）
    分区是针对新字段；分桶针对已有字段
    1）先创建分桶表，通过直接导入数据文件的方式
    > create table stu buck(id int, name string)
    > clustered by(id)
    > into 4 buckets
    > row format delimited fields terminated by '\t';
    
    > desc formatted stu_buck;
    > load data local inpath '/path..' into table stu_back;
    > trancate table 名
    > alter table name drop partition(key=value),partition(key=value),....
    结果没有分桶
    2）先创建一个普通表，然后通过子查询的方法将数据导入分桶表
    需要设置一个属性：set hive.enforce.bucketing=true; set mapreduce.job.reduces=-1;
    再插入数据 insert into table stu_back select id,name from stu;
    
    > select * from stu_buck tablesample(bucket 1 out of 4 on id);
    tablesample(bucket x out of y) y必须是table总桶数的因子或者倍数，决定抽样比例；
    x表示从哪个bucket开始抽取，如果需要抽取多个分区，以后的分区号为当前加上y
    x必须小于y
17. date_format('2019-12-13','yyyy-MM-dd HH:mm:ss')
    date_add('2019-12-13',5/-5)
    date_sub()
    datediff('2019-12-13','2019-12-14')
    regexp_replace('2019/12/13','/','-') 替换
    
    时间比较大小：
    if(cast(date as string)>cast(date2 as string),1,0)
    unix_timestamp(date)-unix_timestamp(date2) 大于0表示date比date2大
    
18. > select concat(deptno,'-',dname) from table; 多列合成一列  合并一行的数据 把多个字段拼成一个
    > select concat_ws('-',field1,field2,...) 必须是string类型field1-field2-field3
    > select collect_set(field)  把该字段的值去重变成一个array 把多行合成一行 是一个聚合函数 需要和group by连用
    > concat_ws('|',collect_set(ti.name)) name group by 把一组的数据化成一行
    > explode(字段） 把字段中每个值拆成多行
    > select movie,category from movie_info lateral view explode(category) table_name as category_name
    把拆分出来的多行与原来的表关联起来
19. hive架构
    元数据 存在mysql中
    原始数据 存在HDFS中
    解析器、编译器、优化器、执行器
    本质：将HQL转化成Mapreduce程序
    mysql安装是服务级别的安装，在root下安装
20. 优化方式
    1）fetch抓取： 全局查找、字段查找、limit查找不走mapreduce 需要设置hive.fetch.task.conversion=more
    2) 本地模式：set hive.exec.mode.local.auto=true, 当输入文件数小于4个，大小小于128M
    3）小表 join 大表： set hive.auto.convert.join = true 会自动对大小表的join做调整
    4）大表 join 大表： 
      空key过滤：
      空key转换：空key的数据很多，但是不能过滤，若不操作会导致数据倾斜，因为同一个key的数据会发给一个reducer，所以可以
      给空key赋随机值
      从输出网址中查看是否出现数据倾斜 
      case when n.id is null then concat('hive',rand()) else n.id end 降低数据倾斜，但是总体时间会变慢
    5）mapjoin：set hive.auto.convert.join = true; set hive.mapjoin.smalltable.filesize=250000000(大小表的阈值设置25M）
      可以用mapjoin把小表全部加载到内存在map端进行join，避免reducer操作
    6）group by: 可以开启map端进行聚合 hive.map.aggr = true; 
                 在map端进行聚合的条目 hive.groupby.mapaggr.checkinterval=100000
                 有数据倾斜的时候进行负载均衡 hive.groupby.skewindata=true：会生成两个MR任务，第一个MR任务中Map的输出结果
                 会随机分到reduce中，每个reduce部分聚合并输出结果，但是想通过key的可能被分到不同reduce中，然后再进入第二个
                 MR任务完成最终聚合
    7）count(distinct) 需要一个reduce完成，需要处理的量比较大，一般先用group by在count的方式
       select count(id) from (select id from table group by id)
       实际中
       ,count(distinct u) as user_pv -- 曝光用户量
        ,count(distinct qpid) as video_pv -- 内容曝光个数
        ,count(1) as pv -- 内容曝光次数
        ,count(1)/count(distinct u) as avg_pv -- 人均曝光次数
        这样直接写有时也可以，如果不行再调整
    8) 避免笛卡儿积：join必须加有效的on,hive只能用一个reducer完成笛卡儿积
    9）行列过滤：只拿需要的列；连表先写where条件
    10）动态分区调整： 
        set hive.exec.dynamic.partition=true;
        set hive.exec.dynamic.partition.mode=nonstrict;  --启用动态分区
        insert into table dept_par partition(deptno) --这里不指定分区名就是动态分区，指定就是静态分区
        select loc,name,deptno from dept; 分区字段根据最后一个select字段决定
    11）分区/分桶
    12）map前将小文件合并 参数已经默认设置
        复杂文件增加map数量
        调整reducer数量  set mapreduce,hob.reduces = 15; 若为-1根据输入的原始数据量/256得到
        hive.exec.reducers.max = 1009最大的reducer数量
21. hadoop压缩
    set hive.exec.compress.intermediate=true; hive中间传输数据压缩功能
    set mapreduce.map.output.compress = true; map端输出压缩功能
    set mapreduce.map.output.compress.codec = org.apache.hadoop.io.compress.SnappyCodec;
    set hive.exec.compress.output=true; hive最终输出数据压缩功能
22. 行存储和列存储 select * select column
23. hive存入数据：load/insert put(修复数据/增加分区）
24. hive会把一个查询转化为多个阶段（mr阶段、抽样阶段、合并阶段、limit阶段),默认会一次执行一个阶段，但是有些可以并行；
    并行执行：set hive.exec.parallel=true； set hive.exeu.parallel.thread.number = 16;同一个sql允许的最大并行度
25. SerDe 允许Hive 从table 读取数据，并以任何自定义格式将其写回HDFS
26. Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能
27. 可以使用explain命令，查看语法树，即hive先将sql语法转成对应的语法树 还可以使用explain extended 查看更多信息
28. COALESCE ( expression,value1,value2……,valuen) 
    COALESCE()函数的第一个参数expression为待检测的表达式，而其后的参数个数不定。
    COALESCE()函数将会返回包括expression在内的所有参数中的第一个非空表达式。
    COALESCE()函数可以用来完成几乎所有的空值处理，不过在很多数据库系统中都提供了它的简化版，这些简化版中只接受两个变量，其参数格式如下： 
    MYSQL: 
      IFNULL(expression,value) 
    MSSQLServer: 
      ISNULL(expression,value) 
    Oracle: 
      NVL(expression,value) 
29. UDF(User-Defined-Function)，一对一。Hive内置字符串，数学函数，时间函数都是
       这种类型。大多数情况下编写对应功能的处理函数就能满足需求。如：concat, split, length ,rand等。
       这种UDF主要有两种写法：继承实现UDF类和继承GenericUDF类（通用UDF）
       
    UDTF(User-Defined Table-Generating Functions) 一对多。
       如explode, 通常配合Lateral View使用，实现列转行的功能。parse_url_tuple将一列转为多列
       
    UDAF(User Defined Aggregation Function)多对一。
       一般配合group by使用。主要用于累加操作，常见的函数有max， min， count， sum，collect_set
       这种UDF主要有两种写法：继承实现 UDAF类和继承实现AbstractGenericUDAFResolver类。
       
  UDF函数可以直接应用于select语句，对查询结构做格式化处理后，再输出内容。
  编写UDF函数的时候需要注意一下几点：
    a）自定义UDF需要继承org.apache.hadoop.hive.ql.UDF。
    b）需要实现evaluate函。 函数名只能使用evaluate
    c）evaluate函数支持重载。
    4）在hive命令行创建函数 添加jar--add jar path 创建function--create [temporary] function add5 as '全类名'
    5）在hive删除函数
 
 UDTF函数
    1）需要继承GenericUDTF类
    2）需要实现三种方法
        @override 表示子类重写了父类的方法 
        inistialize 初始化方法 数据类型与sql不同 输出数据的列名/输出数据的类型定义
        process 业务逻辑：获取数据-获取分隔符-遍历写出-将数据放在集合中-写出forward(datalist)
        close
    3）打成jar包上传
    4）将jar包添加到hive的class path下
    5）创建临时函数与开发好的javaclass关联 

30. 查看hive自有函数：
  show functions --查看已有函数 describe function concat -- 查看函数用法 describe function extended concat -- 查看函数的用户和示例
31. 字符串函数
  LENGTH( string str ) 长度
  TRIM( string str ) 去除空格
  UPPER( string str ), UCASE( string str ) 
  SUBSTR( string source_str, int start_position [,int length]  ) 返回字符串的一部分
  SUBSTRING( string source_str, int start_position [,int length]  ) SUBSTR('hadoop',4) returns 'oop'  SUBSTR('hadoop',4,2) returns 'oo'
  SPLIT( string str, string pat )和python一样分割字符串   SPLIT('hive:hadoop',':') returns ["hive","hadoop"]
  LPAD( string str, int len, string pad ) 用指定字符串在左侧填充到指定长度
  FIND_IN_SET( string search_string, string source_string_list ) 返回目标字符串位置
  CONCAT( string str1, string str2... ) 链接字符串
  CONCAT_WS( string delimiter, string str1, string str2... )  用第一个字符链接后面所有字符串
32. 窗口函数
  over() 给每条每条每条数据都开窗，限定一个聚合函数的作用范围 
    group by 后面才执行窗口函数
    group by只能多对一，无法给每条数据都产生一个结果
    sum() over(distribute by cost sort by orderdate) 按人按日期据累加
    over()其他限制窗口大小的函数————current row当前行 row between  and  current row
    查看每个人的购买总量 select name,orderdate,cost,sum(cost) over(distribute by name sort by orderdate)
    查看每个人上次购买时间 select name,orderdate,cost,lag(orderdate,1,'1970-01-01') over(distribute by name sort by orderdate)
    lead(field,n,default) 后n行的某个数据
    利用lag可以计算两天连续买的人
    ntile(n) 把所有数据行平均分组  
    查看前20% 时间的订单信息 select ntile_5 from (
    select name,cost,orderdate,ntile(5) over(order by orderdate) ntile_5 from business) where ntile_5=1;
  over前面
    rank() 排序相同会重复，总数不变 并列第一，没有第二了
    row_number() 没有并列
    dense_rank() 排序相同会重复，总数会减少，并列第一有第二
 33. hive命令行中可以执行hadoop的命令 dfs
 34. hive数据仓库默认位置： hdfs上：/user/hive/warehouse
     hive-site.xml conf文件夹下面
     hive-default.xml
     -hiveconf param=value 优先级最高
 35. hive数据类型：bigint double string int map<string,int> struct<street:string,city:string> array<string>
 36. DDL 库和表的增删改查 --操作元数据
    1）内外部表
       内部表（管理表）--元数据(mysql中） 能被删除
       外部表--实际数据（hdfs)不能被删除
       转化：> desc formatted student; 查询表类型
             > alter table student set tblproperties('EXTERNAL'='TRUE'/'FALSE'); 区分大小写
    2）建表
       create [external] table [if not exists] tablename [(col type,...)] 
       partitioned by (col type,..)
       row format row_format（默认是SerDe-序列化反序列化)(delimited fileds terminated by char)
       location path (指定表在hdfs的存储位置）
       
       !!!!!!hive存表的时候一定要把列名对应着create建表的列名存，不然数据会对不齐。比如在后面插两列，select的时候也要把这两类
       放在最后两列（除了分区字段）
       
       外部表，仅仅记录数据所在路径，不对数据进行转移 元数据被删除实际数据不动
       内部表，会将数据转移到数据仓库指定路径， 元数据和实际数据一起删除
     3) 修改表
        alter table tblname change column oldname newname type; 改变原有列名或属性
        alter table tblname replace column (newname type); 只能替换所有列
        alter table tblname add columns (col type,col typ...); 增加列，在所有列的后面（partition列前）
        update tblname set col='' [where]
     4）删除表
        truncate table tblname，只能清空管理表--清空表数据
        drop tabel tblname (同mysql) 删除表结构
           
 37. DML 数据操作语言：数据导入/导出 --操作实际数据
     本地插入：load data [local本地/hdfs] inpath 'path' [overwrite覆盖] into table haha; 
     通过查询插入：insert overwrite/into(覆盖/不覆盖） table student partition (dt=) select... insert overwrite ...(可连多个）
     查询创建表并加载数据：create table if not exists student as select name,score from student0; 
     创建表通过location指定加载数据位置：create table tablename() location 'hadoop path';
     import数据到指定hive表：只能对先export的数据导出的
     
     插入 静态分区 select后不能加分区字段
          动态分区 select后需要加分区字段，必须保证是最后一个

 38.数据导出：
    insert导出： insert overwrite [local] directory 'path' row format delimited fields terminated by '\t' 
    导出到本地： hive -e '' > path
 39.truncate 清空表，只能清空管理表
 40.分区表：
    对应一个HDFS文件系统上独立的文件夹，文件夹下面是该分区的所有数据文件
    Hive分区就是分目录
    谓词下推：先走where再选择
    > alter table tblname add partition(dt='2019-12-12') partition(dt='2019-12-14')
    > alter table tblname drop partition(dt=''),partition(dt='');
    > show partitions tblname;
    > desc formatted tblname
    
    二级分区：
    partitioned by (col type1,col2 type2);
    三种上传数据添加分区的方式：
    1）上传后修复： > dfs -mkdir -p path 建立文件夹
                   > dfs -put source target 添加原始数据
                   > msck repair table tblname; 添加元数据信息
    2) 上传后添加分区：前两步同上
                   > alter table tblname add partition(); 添加分区
    3）创建文件夹后load数据到分区：
                    > load data local inpath  into table tblname partition();
    4) 建立一个分区表，然后直接insert数据到分区：
                    > insert overwrite table tblname partition(dt='') select...;
41. 分位数
    将改组数据排序以后用小于等于该数的个数/总的数字的个数
    percentile(col,p) 只能对整数列求,p表示第p百分位数（一组n个观测值按数值大小排列。如，处于p%位置的值称第p百分位数）
    percentile_approx(col,array(p1,p2,p3..),B)
    95th percentile百分点指的是所给数集中超过其95%的数
    中位数是第50百分位数
    第25百分位数又称第一个四分位数（First Quartile），用Q1表示；
    第50百分位数又称第二个四分位数（Second Quartile），用Q2表示；
    第75百分位数又称第三个四分位数（Third Quartile）,用Q3表示

42. 计算次留：一是每天算7天的，更改这7天的数据,分区要设置成不严格的
              二是存一张remain的表，然后存一个days 和 from_date
              
43. 这种方法可以使得在计算的时候同时统计每一类的值和总计，也可以使用grouping sets或者with cube
   from (
        select A.*,'ALL' as ab
        from user_detail_mix A 

        union all
        select A.*,B.ab as ab
        from user_detail_mix A 
        join qube_base_pps_d_user_ab_tag_tbl B on B.dt='${dt_1}' and B.u=A.u
    ) A
    group by user_type,platform,ab,channelname
44. 把一个表从一个数据库复制到另一个数据库
  create table db1.table1 as select * from db2.`table2` ;   
  navicat-工具-数据传输

45.hive
   Hive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为 一张数据库表，并提供类 SQL 查询功能。本质是将 SQL 转换为
   MapReduce 程序。 主要用途：用来做离线数据分析，比直接用 MapReduce 开发效率更高。
   
   Hive 中所有的数据都存储在 HDFS 中，没有专门的数据存储格式
   只需要在创建表的时候告诉 Hive 数据中的列分隔符和行分隔符，Hive 就 可以解析数据。
   
   Hive 中包含以下数据模型：DB、Table，External Table，Partition，Bucket。
      db：在 hdfs 中表现为${hive.metastore.warehouse.dir}目录下一个文件夹
      table：在 hdfs 中表现所属 db 目录下一个文件夹
      external table：与 table 类似，不过其数据存放位置可以在任意指定路径
      partition：在 hdfs 中表现为 table 目录下的子目录
      bucket：在 hdfs 中表现为同一个表目录下根据 hash 散列之后的多个文件

46.hive和一般数据库不同
   1）查询语言。由于 SQL 被广泛的应用在数据仓库中，因此，专门针对 Hive 的特性设计了类 SQL 的查询语言 HQL。
   2）数据存储位置。Hive 是建立在 Hadoop 之上的，所有 Hive 的数据都是存储在 HDFS 中的。而数据库则可以将数据保存在块设备或者本地文件系统中。
   3)数据格式。Hive 中没有定义专门的数据格式，数据格式可以由用户指定，用户定义数据格式需要指定三个属性：列分隔符（通常为空格、”\t”、
   ”\x001″）、行分隔符（”\n”）以及读取文件数据的方法（Hive 中默认有三个文件格式 TextFile，SequenceFile 以及 RCFile）。由于在加载
   数据的过程中，不需要从用户数据格式到 Hive 定义的数据格式的转换，因此，Hive 在加载的过程中不会对数据本身进行任何修改，而只是将数
   据内容复制或者移动到相应的 HDFS 目录中。而在数据库中，不同的数据库有不同的存储引擎，定义了自己的数据格式。所有数据都会按照一定的
   组织存储，因此，RDBMS数据库加载数据的过程会比较耗时。
   4)数据更新。由于 Hive 是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive 中不支持对数据的改写和添加，所有的数据
   都是在加载的时候中确定好的。而数据库中的数据通常是需要经常进行修改的，
   5)索引。Hive 在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些 Key 建立索引。
   Hive 要访问数据中满足条件的特定值时，需要暴力扫描整个数据，因此访问延迟较高。由于 MapReduce 的引入， Hive 可以并行访问数据，因此
   即使没有索引，对于大数据量的访问，Hive 仍然可以体现出优势。数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数
   据的访问，数据库可以有很高的效率，较低的延迟。由于数据的访问延迟较高，决定了 Hive 不适合在线数据查询。
   6)执行。Hive 中大多数查询的执行是通过 Hadoop 提供的 MapReduce 来实现的（类似 select * from tbl 的查询不需要 MapReduce）。而数据库通
   常有自己的执行引擎。
   7)执行延迟。之前提到，Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive 执行延迟高的因素是 
   MapReduce 框架。由于 MapReduce 本身具有较高的延迟，因此在利用 MapReduce 执行 Hive 查询时，也会有较高的延迟。相对的，数据库的
   执行延迟较低。当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive 的并行计算显然能体现出优势。
   hive执行延迟高，只有在数据规模达到一定程度后，其查询的高效才能弥补其高延迟的劣势。
   8)可扩展性。由于 Hive 是建立在 Hadoop 之上的，因此 Hive 的可扩展性是和 Hadoop 的可扩展性是一致的（世界上最大的 Hadoop 集群在 
   Yahoo!，2009年的规模在 4000 台节点左右）。而数据库由于 ACID 语义的严格限制，扩展行非常有限。目前最先进的并行数据库 Oracle 在理
   论上的扩展能力也只有 100 台左右。
   9)数据规模。由于 Hive 建立在集群上并可以利用 MapReduce 进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据
   规模较小。
 
 47. Hive的分区表
   在Hive Select查询中一般会扫描整个表内容，会消耗很多时间做没必要的工作。有时候只需要扫描表中关心的一部分数据，因此建表时引入了partition概念。
   分区表指的是在创建表时指定的partition的分区空间。
   如果需要创建有分区的表，需要在create表的时候调用可选参数partitioned by，详见表创建的语法结构
   在 Hive 中，表中的一个 Partition 对应于表下的一个目录，所有的 Partition 的数据都存储在最字集的目录中
  
